{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define data root directory\n",
    "data_dir = \"/home/jintaek/Desktop/Nas/John_VR_Study/New_OutputData_x\"\n",
    "path = \"New_OutputData_x\"\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"  # Set the GPUs 2 and 3 to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_dir, lookback):\n",
    "    label_scalers = {}\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = {}\n",
    "    test_y = {}\n",
    "\n",
    "    # Use tqdm for progress indication\n",
    "    for file in tqdm(os.listdir(data_dir)):  # Use standard tqdm here\n",
    "        # Skip files that don't match criteria\n",
    "        if not file.endswith(\".csv\") or file == \"pjm_hourly_est.csv\":\n",
    "            continue\n",
    "\n",
    "        # Store csv file in a Pandas DataFrame\n",
    "        df = pd.read_csv(os.path.join(data_dir, file))\n",
    "        df.columns = ['time', 'pos', 'acc', 'ref']\n",
    "        df = df[['ref', 'time', 'pos', 'acc']]\n",
    "\n",
    "        # Check if the dataset is large enough\n",
    "        if len(df) <= lookback:\n",
    "            # print(f\"Skipping {file} due to insufficient data length ({len(df)} rows).\")\n",
    "            continue\n",
    "\n",
    "        # Scaling the input data\n",
    "        sc = MinMaxScaler()\n",
    "        label_sc = MinMaxScaler()\n",
    "        data = sc.fit_transform(df.values)\n",
    "        \n",
    "        # Obtaining the Scale for the labels (usage data) so that output can be re-scaled to actual value during evaluation\n",
    "        label_sc.fit(df.iloc[:, 0].values.reshape(-1, 1))\n",
    "        label_scalers[file] = label_sc\n",
    "\n",
    "        # Define lookback period and split inputs/labels\n",
    "        inputs = np.zeros((len(data) - lookback, lookback, df.shape[1]))\n",
    "        labels = np.zeros(len(data) - lookback)\n",
    "\n",
    "        for i in range(lookback, len(data)):\n",
    "            inputs[i - lookback] = data[i - lookback:i]\n",
    "            labels[i - lookback] = data[i, 0]\n",
    "\n",
    "        inputs = inputs.reshape(-1, lookback, df.shape[1])\n",
    "        labels = labels.reshape(-1, 1)\n",
    "\n",
    "        # Split data into train/test portions and combine all data from different files into a single array\n",
    "        test_portion = int(0.1 * len(inputs))\n",
    "        if len(train_x) == 0:\n",
    "            train_x = inputs[:-test_portion]\n",
    "            train_y = labels[:-test_portion]\n",
    "        else:\n",
    "            train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
    "            train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
    "        \n",
    "        test_x[file] = inputs[-test_portion:]\n",
    "        test_y[file] = labels[-test_portion:]\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return train_loader, test_x, test_y, label_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=5, EPOCHS=20, model_type=\"GRU\"):\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 3\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "\n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    LossEpoch = []\n",
    "    TotalLossEpoch = []\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        start_time = time.perf_counter()  # Use time.perf_counter() for more precise timing\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.0\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "\n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter % 200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss / counter))\n",
    "        \n",
    "        current_time = time.perf_counter()  # Use time.perf_counter() for more precise timing\n",
    "        # print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss / len(train_loader)))\n",
    "        # print(\"Total Time Elapsed: {} seconds\".format(str(current_time - start_time)))\n",
    "        epoch_times.append(current_time - start_time)\n",
    "        LossEpoch.append(avg_loss / counter)\n",
    "        TotalLossEpoch.append(avg_loss / len(train_loader))\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model, LossEpoch, TotalLossEpoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(lstm_losses, gru_losses, lookback, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(lstm_losses, label='LSTM Loss')\n",
    "    plt.plot(gru_losses, label='GRU Loss')\n",
    "    plt.title(f'Loss per Epoch (Lookback={lookback})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the plot with a specific filename in the output directory\n",
    "    plt.savefig(os.path.join(output_dir, f'loss_plot_lookback_{lookback}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.perf_counter()\n",
    "    for i in test_x.keys():\n",
    "        inp = torch.from_numpy(np.array(test_x[i]))\n",
    "        labs = torch.from_numpy(np.array(test_y[i]))\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    print(\"Evaluation Time: {}\".format(str(time.perf_counter()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(abs(outputs[i])+abs(targets[i])))\n",
    "    return 200*sMAPE/len(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1350/1350 [00:51<00:00, 26.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Total Training Time: 26.239873252809048 seconds\n",
      "Starting Training of LSTM model\n",
      "Total Training Time: 25.58194123953581 seconds\n",
      "\n",
      "*****LOOKBACK = 50*******\n",
      "GRU initial loss: 0.0754\n",
      "GRU final loss: 0.0039\n",
      "LSTM initial loss: 0.8471\n",
      "LSTM final loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1350/1350 [00:27<00:00, 48.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Total Training Time: 20.89996663480997 seconds\n",
      "Starting Training of LSTM model\n",
      "Total Training Time: 20.71582391113043 seconds\n",
      "\n",
      "*****LOOKBACK = 100*******\n",
      "GRU initial loss: 0.3146\n",
      "GRU final loss: 0.0051\n",
      "LSTM initial loss: 0.1448\n",
      "LSTM final loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "lookback_values = [50, 100]\n",
    "lr = 0.001  # Learning rate\n",
    "\n",
    "# Directory to save plots\n",
    "plot_output_dir = \"/home/jintaek/Desktop/Nas/VR_control_Code/VR_control/Plot\"\n",
    "\n",
    "for lookback in lookback_values:\n",
    "    # Prepare data with the current lookback\n",
    "    train_loader, test_x, test_y, label_scalers = prepare_data(data_dir, lookback)\n",
    "    \n",
    "    # Train the GRU model\n",
    "    gru_model, gru_average_loss_epoch, gru_total_loss_epoch = train(train_loader, lr, model_type=\"GRU\")\n",
    "\n",
    "    # Train the LSTM model\n",
    "    lstm_model, lstm_average_loss_epoch, lstm_total_loss_epoch = train(train_loader, lr, model_type=\"LSTM\")\n",
    "\n",
    "    # Print initial and final losses for GRU model\n",
    "    print(f\"\\n*****LOOKBACK = {lookback}*******\")\n",
    "    print(f\"GRU initial loss: {gru_total_loss_epoch[0]:.4f}\")\n",
    "    print(f\"GRU final loss: {gru_total_loss_epoch[-1]:.4f}\")\n",
    "\n",
    "    # Print initial and final losses for LSTM model\n",
    "    print(f\"LSTM initial loss: {lstm_total_loss_epoch[0]:.4f}\")\n",
    "    print(f\"LSTM final loss: {lstm_total_loss_epoch[-1]:.4f}\")\n",
    "\n",
    "    # Plotting the losses\n",
    "    plot_losses(lstm_average_loss_epoch, gru_average_loss_epoch, lookback, plot_output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
